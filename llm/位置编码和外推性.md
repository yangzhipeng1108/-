https://zhuanlan.zhihu.com/p/653081131

https://zhuanlan.zhihu.com/p/675243992
位置线性内插（Position Interpolation，PI）  训练轮次少 985.5 > 984 容易训练  主要集中末位变化 有偏移
NTK-Aware Scaled RoPE  

https://zhuanlan.zhihu.com/p/650469278

https://xueqiu.com/6597170251/198578843


RoPE使用一个旋转矩阵，将每个位置的位置向量与旋转矩阵相乘，从而获得旋转后的位置向量。这样，模型可以根据旋转后的位置向量来识别和理解位置之间的旋转模式。



4.3 旋转位置编码 RoPE 有什么优点？
旋转位置编码（RoPE）是一种用于位置编码的改进方法，相比于传统的位置编码方式，RoPE具有以下优点：
解决位置编码的周期性问题：传统的位置编码方式（如Sinusoidal Position Encoding）存在一个固定的周期，当序列长度超过该周期时，位置编码会出现重复。
这可能导致模型在处理长序列时失去对位置信息的准确理解。RoPE通过引入旋转操作，可以解决这个周期性问题，使得位置编码可以适应更长的序列。
更好地建模相对位置信息：传统的位置编码方式只考虑了绝对位置信息，即每个位置都有一个唯一的编码表示。然而，在某些任务中，相对位置信息对于理解序列的语义和结构非常重要。
RoPE通过旋转操作，可以捕捉到相对位置信息，使得模型能够更好地建模序列中的局部关系。
更好的泛化能力：RoPE的旋转操作可以看作是对位置编码进行了一种数据增强操作，通过扩展位置编码的变化范围，可以提高模型的泛化能力。
这对于处理不同长度的序列以及在测试时遇到未见过的序列长度非常有帮助。
总体而言，RoPE相比于传统的位置编码方式，在处理长序列、建模相对位置信息和提高泛化能力方面具有一定的优势。这些优点可以帮助模型更好地理解序列数据，
并在各种自然语言处理任务中取得更好的性能。

4.4 旋转位置编码 RoPE 被哪些 LLMs 应用？
5 长度外推问题篇
5.1 什么是 长度外推问题？
长度外推问题是指在机器学习和自然语言处理中，模型被要求在输入序列的长度超出其训练范围时进行预测或生成。这种情况下，模型需要推断或生成与其训练数据中的示例长度不同的序列。

长度外推问题通常是由于训练数据的限制或资源限制而引起的。例如，在语言模型中，模型可能只能训练到一定长度的句子，但在实际应用中，
需要生成更长的句子。在这种情况下，模型需要学会推断和生成超出其训练数据长度范围的内容。

解决长度外推问题的方法包括使用合适的编码器和解码器架构，使用适当的位置编码方法（如RoPE），以及训练模型时使用更大的输入序列范围。
此外，还可以使用基于生成模型的方法，如生成对抗网络（GAN），来生成更长的序列。长度外推问题是自然语言处理中一个重要的挑战，对于实现更强大的语言模型和生成模型具有重要意义。

5.2 长度外推问题 的 解决方法 有哪些？
解决长度外推问题的方法主要包括以下几种：

使用适当的模型架构：选择能够处理不同长度序列的模型架构。例如，Transformer模型在处理长度变化的序列时表现出色，因为它使用自注意力机制来捕捉序列中的长距离依赖关系。
使用适当的位置编码方法：为了帮助模型理解序列中不同位置的信息，可以使用位置编码方法，如相对位置编码（RoPE）或绝对位置编码。
这些编码方法可以帮助模型推断和生成超出其训练范围的序列。
增加训练数据范围：如果可能，可以增加训练数据的范围，包括更长的序列示例。这样可以让模型更好地学习如何处理超出其训练范围的序列。
使用生成模型：生成模型如生成对抗网络（GAN）可以用于生成更长的序列。GAN模型可以通过生成器网络生成超出训练数据范围的序列，并通过判别器网络进行评估和优化。
增加模型容量：增加模型的容量（如增加隐藏层的大小或增加模型的参数数量）可以提高模型处理长度外推问题的能力。更大的模型容量可以更好地捕捉序列中的复杂模式和依赖关系。
使用迭代方法：对于超出模型训练范围的序列，可以使用迭代方法进行外推。例如，可以通过多次迭代生成序列的一部分，并将生成的部分作为下一次迭代的输入，从而逐步生成完整的序列。
这些方法可以单独或组合使用来解决长度外推问题，具体的选择取决于具体的任务和数据。

6 ALiBi (Attention with Linear Biases)篇
6.1 ALiBi (Attention with Linear Biases) 思路是什么？
ALiBi（Attention with Linear Biases）是一种用于处理长度外推问题的方法，它通过引入线性偏置来改进自注意力机制（Self-Attention）。

自注意力机制是一种用于捕捉序列中不同位置之间依赖关系的机制，它通过计算每个位置与其他位置的注意力权重来加权聚合信息。
然而，自注意力机制在处理长度变化的序列时存在一些问题，例如在处理长序列时，注意力权重可能变得过于稀疏或集中，导致模型无法有效地捕捉长距离依赖关系。

ALiBi的思路是在自注意力机制中引入线性偏置，以增强模型对长距离依赖关系的建模能力。具体来说，ALiBi使用线性映射将输入序列转换为一个低维度的特征向量，
然后通过计算特征向量之间的内积来计算注意力权重。这样做的好处是，线性映射可以将输入序列的信息压缩到一个更紧凑的表示中，从而减少模型对长距离依赖关系的建模难度。

ALiBi还引入了一个线性偏置向量，用于调整注意力权重的分布。通过调整偏置向量的值，可以控制注意力权重的稀疏性和集中性，从而更好地适应不同长度的序列。
这种线性偏置的引入可以帮助模型更好地处理长度外推问题，提高模型在处理长序列时的性能。

总的来说，ALiBi通过引入线性偏置来改进自注意力机制，增强模型对长距离依赖关系的建模能力，从而提高模型在处理长度外推问题时的性能。

6.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？
在ALiBi中，偏置矩阵是一个用于调整注意力权重的矩阵。具体来说，偏置矩阵是一个形状为（L，L）的矩阵，其中L是输入序列的长度。矩阵中的每个元素都是一个偏置值，
用于调整注意力权重的分布。

偏置矩阵的作用是在计算注意力权重时引入一个额外的偏置项，从而调整注意力的分布。通过调整偏置矩阵的值，可以控制注意力权重的稀疏性和集中性，以更好地适应不同长度的序列。

具体来说，偏置矩阵通过与注意力权重矩阵相乘，对注意力权重进行调整。偏置矩阵中的每个元素与注意力权重矩阵中的对应元素相乘，可以增加或减小该位置的注意力权重。
通过调整偏置矩阵的值，可以控制不同位置的注意力权重，使其更加稀疏或集中。

偏置矩阵的引入可以帮助模型更好地处理长度外推问题。通过调整注意力权重的分布，模型可以更好地适应不同长度的序列，并更好地捕捉序列中的长距离依赖关系。
偏置矩阵提供了一种灵活的方式来控制注意力权重的调整，从而提高模型在处理长度外推问题时的性能。

6.3 ALiBi (Attention with Linear Biases) 有什么优点？
ALiBi（Attention with Linear Biases）具有以下几个优点：

改善了自注意力机制的性能：ALiBi通过引入线性偏置来改进自注意力机制，增强了模型对长距离依赖关系的建模能力。这样可以更好地捕捉序列中的长距离依赖关系，提高模型的性能。
灵活性：ALiBi中的偏置矩阵提供了一种灵活的方式来调整注意力权重的分布。通过调整偏置矩阵的值，可以控制注意力权重的稀疏性和集中性，
以更好地适应不同长度的序列。这种灵活性使得ALiBi能够适应不同的任务和数据特点。
减少参数数量：ALiBi使用线性映射将输入序列转换为一个低维度的特征向量，从而减少了模型的参数数量。这样可以降低模型的复杂度，减少计算和存储成本，并提高模型的效率。
通用性：ALiBi可以应用于各种长度外推问题，如序列预测、机器翻译等。它的思路和方法可以适用于不同领域和任务，具有一定的通用性。

ALiBi中的偏置矩阵提供了一种灵活的方式来调整注意力权重的分布。