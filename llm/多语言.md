可以看出，基于7B版本BLOOM的结果比13B版本LLaMA还好了不少。

2. 多语言指令微调
为了能够在指令微调阶段进行多语言的训练，我们还需要收集多语言的指令微调数据。总体而言，可以有两条路径。
2.1 指令微调数据的翻译
指令微调数据的翻译可以分为两种形式：
（1）直接翻译：直接将现有的数据集进行翻译，例如，我们可以直接把现有的alpaca数据（指令以及回答）翻译至其他语言。这种方式的优点是简单。
然而这可能会造成一定问题，比如，我们让ChatGPT讲一个英文谐音梗笑话，如果直接翻译到中文，这个谐音梗可能就不成立了。
（2）先翻译指令后生成：在这种方式中，我们只需要将指令翻译成我们想要的语言，然后让ChatGPT重新生成一遍答案，这样类似谐音梗这种问题可能得到一定程度上的解决。
2.2 天然的多语言聊天数据 -- 聊天数据
目前，有不少平台对ChatGPT进行了代理，让用户更加方便地使用ChatGPT，这其中就形成了很多天然的多语言数据（因为用户来源于各个国家）。
这方面数据是多语言的一大重要数据来源之一。

3. 结论
为了能够让大语言模型支持多语言，需要在预训练以及指令微调两个阶段都下功夫，两者缺一不可。

mbert
本文提出的Multilingual BERT训练方法很简单，使用来自104种语言的单语语料（使用shared word piece vocabulary），采用BERT的训练目标(MLM)进行训练，
训练过程中没有加入任何信息来指示每句话的语种，也没有显性的机制来促使不同语言里面的同义句去拥有相似的表达。
也就是说，对于模型来说，它只学习了一种“大杂烩”语言，其训练方式和训练一个英语的BERT没有任何区别。
【浅层】---> 由于某些语言之间共享词表，mBERT直接将这样的语言混合在一起训练，模型自然捕获到一些浅层表示的关联
【深层】---> 不同语言之间存在语法和语义层面的关联

PolyLM
大型语言模型 (LLM) 展示了出色的遵从自然语言指令理解、推理和生成的能力。然而，开发LLMs主要集中在高资源语言，例如英语，从而限制了它们在其他语言中的应用和研究。
因此，我们开发了PolyLM，一个在6400亿个词的数据上从头训练的多语言语言模型，包括两种模型大小(1.7B和13B)。
PolyLM覆盖中、英、俄、西、法、葡、德、意、荷、波、阿、土、希伯来、日、韩、泰、越、印尼等语种，特别是对亚洲语种更友好。
为了增强多语言能力，我们 1) 将双语数据集成到训练数据; 和2)采用一种课程学习策略，在预训练期间将非英语数据的比例从30%提高到60%。
此外，我们提出了一种多语言自指令方法，自动为模型生成132.7K条多样的多语言指令。为了评估模型的效能，我们收集了几种现成的多语言任务，
包括多语言理解、问答、生成和翻译。广泛的实验表明，PolyLM超过了其他开源模型，如LLaMA和BLOOM，在多语言任务中表现良好，同时在英语上保持了可比较的性能。

b. 一种课程学习策略以缓解小语种数据规模和质量不足的问题：
考虑到英语语言数据规模大，质量高，知识丰富，我们在预训练初期英语数据规模达到近70%， 这个比例随着训练调整，在最后阶段英语比例下降到40%，
通过这种方式把常识知识向小语种转移。我们在训练中引入了近一亿条高质量多语言双语平行数据， 方便训练过程中小语种向高资源语种对齐。
c. 一套从预训练、到指令微调、再到评测多语言LLM的完整基础设施：
在指令微调方面我们从175个英文种子数据出发， 利用大模型的创造力和多样性自动生成了13万条多语言指令数据。


BLOOM
https://zhuanlan.zhihu.com/p/603518061