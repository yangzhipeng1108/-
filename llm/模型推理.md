https://zhuanlan.zhihu.com/p/655325832

vllm pageattention
PageAttention命名的灵感来自OS系统中虚拟内存和分页的思想。可以实现在不连续的空间存储连续的kv键值。
kuberay

kv cache是为了避免每次采样token时重新计算键值向量。利用预先计算好的k值和v值，可以节省大量计算时间，尽管这会占用一定的存储空间。

kv cache

MHA\GQA\MQA优化技术

接下来是GQA和MQA优化技术，在LLAMA2的论文中，提到了相关技术用来做推理优化，目前GQA和MQA也是许多大模型推理研究机构核心探索的方向。

MQA，全称 Multi Query Attention, 而 GQA 则是前段时间 Google 提出的 MQA 变种，全称 Group-Query Attention。
MHA（Multi-head Attention）是标准的多头注意力机制，h个Query、Key 和 Value 矩阵。
MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。
GQA将查询头分成N组，每个组共享一个Key 和 Value 矩阵


FlashAttention优化技术
Flash attention推理加速技术是利用GPU硬件非均匀的存储器层次结构实现内存节省和推理加速
Flash attention的核心原理是尽可能地合理应用SRAM内存计算资源。