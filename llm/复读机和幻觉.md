https://mp.weixin.qq.com/s/mLmL19FI-ZpX9BMJWpY2rw

出现LLMs复读机问题可能有以下几个原因：

数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。
训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。
缺乏多样性的训练数据：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。
模型结构和参数设置：大型语言模型的结构和参数设置也可能对复读机问题产生影响。例如，模型的注意力机制和生成策略可能导致模型更倾向于复制输入的文本。
为了解决复读机问题，可以采取以下策略：

多样性训练数据：在训练阶段，尽量使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。
引入噪声：在生成文本时，可以引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。
温度参数调整：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性，从而减少复读机问题的出现。
后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。


多样性训练数据：在训练阶段，使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。这可以包括从不同领域、不同来源和不同风格的文本中获取数据。
引入噪声：在生成文本时，引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。这可以通过在生成过程中对模型的输出进行采样或添加随机性来实现。
温度参数调整：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性。较高的温度值会增加随机性，从而减少复读机问题的出现。
Beam搜索调整：在生成文本时，可以调整Beam搜索算法的参数。Beam搜索是一种常用的生成策略，它在生成过程中维护了一个候选序列的集合。通过调整Beam大小和搜索宽度，可以控制生成文本的多样性和创造性。
后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。可以使用文本相似度计算方法或规则来检测和去除重复的文本。
人工干预和控制：对于关键任务或敏感场景，可以引入人工干预和控制机制，对生成的文本进行审查和筛选，确保生成结果的准确性和多样性。

幻觉问题
https://zhuanlan.zhihu.com/p/662550362
https://zhuanlan.zhihu.com/p/677935286
1. Intrinsic： 即生成结果与信息源（source）或上下文不一致。例如，生成的摘要或翻译结果与原文矛盾。 这种可以称为"自相矛盾"的幻觉。
2. Extrinsic：即生成结果超出了信息源（source）或上下文的范围，并且难以验证真假。例如，摘要结果中出现了原文没有提及的陈述。这种可以称为"无中生有"的幻觉。

1数据
1.source 与 target 数据分布不一致。target 中存在的信息在 source 中并未出现。
2.训练数据中存在重复时，容易让大模型偏好生成重复的片段。
3.数据清洗工作没有做好，
2. 模型
模型方面的导致幻觉的一个主要因素是训练和推理的 exposure bias 问题。
表征学习存在缺陷。当模型对输入文本的表征能力不够，缺乏篇章级的理解和推理能力，则所生成的表征未能充分表达输入文本的信息，则模型在处理依赖于篇章级信息的任务时容易出现幻觉，即不能忠实于输入原文，造成 intrinsic hallucination。
生成模型的 decoder 也是导致幻觉的一个因素。decoder 的attention模块对输入文本表征的关注重点错了，容易导致模型生成错误。
decoder 采用的一些解码策略也会导致幻觉，例如当采用 top-p sampling 这类策略鼓励模型生成结果的多样性时，不可避免地增加幻觉的发生概率。
大模型通过训练会将学习到的知识以模型参数形式进行记忆或存储，这称之为 parametric knowledge 。当模型在处理下游任务时偏重 parametric knowledge 而不是当前输入的上下文时，容易导致幻觉问题，这种情况被称为 parametric knowledge bias [5]。
1.数据层面
针对大模型幻觉的产生原因，在数据层面的相应解决方案就是通过数据清洗优化训练数据，避免因数据中的噪音或数据bias导致模型的幻觉问题。
如前所述，数据层面可能的一个主要问题是因启发式构造策略或数据自身的特点导致targe信息并未在source中出现。
针对这一问题，一种思路是通过人工标注方式获得质量更高的数据集，另一种是通过启发式规则或者训练判别模型对现有的数据集进行质量判断，过滤掉可能存在问题的样本。
数据层面的其他问题，如数据重复、事实错误等问题，也依赖于通过人工或模型的手段去识别并过滤噪声。

1 增加反馈 强化学习
2. 增加知识  外挂知识库 在模型预测时给模型输入更多的正确知识，也可以有效帮助模型降低幻觉。
3. 增加约束   
增加约束是在翻译任务上的常见方法。其基本思想是，对标准的 beam-search 进行修改，要求模型输出结果包含指定词语[10]。当要求结果忠实于原文时，
通过这种方式可以让模型输出的关键信息如实体名称、事实等可控。针对大模型，另一种增加约束的方法是通过在 prompt 中详细描述对输出结果的约束条件，
让生成结果更符合预期。
4. 优化表征学习  多任务学习

3.后处理
后处理方法是在大模型生成结果的基础上进一步验证和修改。
一种方法是构造包含一定比例幻觉的训练数据（主要针对实体、数字、时间等常见错误类型构造），训练模型根据原文和有错误的生成结果还原出正确结果。
另一种是利用知识图谱解决对话中的幻觉问题，包括2个部分，一个是 critic 进行 token 级的幻觉检查，并将存在幻觉的部分 mask 后输出作为 query，
一个是 retriever 基于知识库查询 query，并得到更可信的实体。

https://mp.weixin.qq.com/s/ktZEDVytrhulry5sRTpN9A


layernorm
https://zhuanlan.zhihu.com/p/479860623
https://zhuanlan.zhihu.com/p/620297938
