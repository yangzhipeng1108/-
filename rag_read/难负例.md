难负例  hard example mining
对分类器迷惑性大的样本，这类样本的实际标签是负的，但是分类器往往预测为正的。negative即负样本， hard说明是困难样本，也可以说是容易将负样本看成正样本的那些样本
https://zhuanlan.zhihu.com/p/384463509

https://aistudio.baidu.com/projectdetail/6507483?channelType=0&channel=0

Baseline	标准 pair-wise 训练范式，通过随机采样产生负样本
In-batch negatives	在 Batch 内同时使用 batch_size 个负样本进行训练
HardestNeg	在 Batch 内先挖掘最难负样本，然后进行 pair-wise 训练

1.1 In-batch negatives 核心思路
In-batch negatives 策略核心是在 1 个 Batch 内同时基于 N 个负例进行梯度更新，
将Batch 内除自身之外其它所有 Source Text 的相似文本 Target Text 作为负例，
例如: 上例中 我手机丢了，我想换个手机 有 1 个正例(1.我想买个新手机，求推荐)，2个负例(1.手机学日语的软件，2.侠盗飞车罪恶都市怎么改车)。

1.2 HardestNeg 核心思路
HardestNeg 策略核心是在 1 个 Batch 内的所有负样本中先挖掘出最难区分的负样本，基于最难负样本进行梯度更新。
例如: 上例中 Source Text: 我手机丢了，我想换个手机 有 2 个负例(1.手机学日语的软件，2.侠盗飞车罪恶都市怎么改车)，
其中最难区分的负例是 手机学日语的软件，模型训练过程中不断挖掘出类似这样的最难负样本，然后基于最难负样本进行梯度更新。